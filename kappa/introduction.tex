\part{Introduction}
\chapter{Introduction}
\section{Introduction}

Program Analysis is the branch of Computer Science that investigates the behaviour
and characteristics of a computer program. Program analysis applications are widely
used in numerous areas of Computer Science, including compiler construction,
e.g., to perform optimisations; software engineering, e.g., to detect bugs or
code smells; security, e.g., to detect vulnerabilities; safety
critical systems, e.g., avionic, automotive, and medical, to detect safety
violations, and many others.
We can distinguish between static and dynamic program analysis techniques.

Dynamic analysis involves executing the program under analysis and gathering
detailed information about its behaviour. Widely used dynamic analysis applications
are, for instance, tracing, profiling, and step-by-step debugging. More sophisticated dynamic
analysis techniques are based on symbolic execution, which consists in executing
the program under analysis with symbolic inputs and then solving the resulting
constraints to obtain concrete values for the program variables. Symbolic execution
is used, for instance, to detect buffer overflows, perform program verification,
and perform program synthesis.
Negatively, the properties verified using dynamic analysis techniques 
are dependent on the parameter used to execute the application. For example, if the
program under analysis is a web server, the properties verified by dynamic analysis
are dependent on the requests issued to the web server. Moreover, dynamic analysis
techniques are not able to detect properties that are not reachable, i.e., dead code.


Static program analysis, static analysis for short, is the practice of reasoning
about a program's behaviour without executing it. The analysis is performed by a static analysis tool,
which is a program that takes as input the source code of other programs and
generates a set of properties that hold for the program being analysed as output.
The properties verified by static analysis techniques are independent of the input
used to execute the program under analysis. Moreover, static analysis techniques can analyse unreachable code.
The main limitations of static analysis techniques are the results of the Turing and Rice undecidability
theorems, i.e., it is impossible to perform sound and complete static analysis of a program for non-trivial properties, e.g., 
we cannot always prove whether a program terminates or not. However, it is possible
to approximate the analyses introducing imprecision, e.g., adding a new possible answer
``I do not know'' to  ``It terminates'' or ``It does not terminate''. The introduction 
of imprecision is a trade-off between the precision of the analysis and the complexity
of the analysis. The more precise the analysis, the more complex it is. 
The more imprecise the analysis, the higher the probability of reporting 
false positives and false negatives to the user. 


Among the numerous approaches to static program analysis, dataflow analysis stands out.
Dataflow analysis is utilised to examine the data dependencies between programme entities
(such as variables, expressions, and statements) in order to infer and collect constraints
on the values of the program's entities. Dataflow applications include, for instance,
constant propagation, registers optimisations, and, null-pointer exception detection.
Dataflow analysis is performed on the program's control-flow graph (CFG), which is a
directed graph where the nodes represent the program statements or expressions,
and the edges reflect the program control-flow. In most cases, the CFG is constructed 
from an intermediate representation of the program's source code (IR).
Different IRs are emplyed for different purposes. For instance, the Abstract Syntax Tree (AST), which
is the tree representation of the program constructed by the parser, is used to perform semantics checks, e.g.,
name analysis and type analysis. Wheras, more low-level IRs, such as the Three Address Code (TAC),
are used to perform optimisations, e.g., registers allocations and dead code elimination. The advantage
of building the CFG on a low-level IR is that the CFG is more compact and requires 
less engineering effort to build it, in that, the implementation can be reused by each 
compiler front-end that targets the same IR.
% The main disadvantage is that the information
% contained in the IR is not easy to map back to the original source code and, therefore,
% the analysis results are not easy to understand by the user.







Like any other software project, the development of a static analysis tool
it is a complex process that involves several steps, including the design of 
flexible and extensible architecture, i.e., a software architecture that is 
easy to extend and maintain. In this thesis, we employ the Declarative Programming
paradigm to overcome this issue. Declarative Programming is a programming 
paradigm that allows the programmer to specify what are the properties of a
program without specifying how to compute them (i.e., without specifying the 
algorithm). We used as declarative formalism an extension of Attribute Grammars. Attribute Grammars
were first introduced in 1968 by Donald Knuth and are a formalism for the specification
of the semantics of programming languages. Attribute Grammars are a generalisation
of Context-Free Grammars, which are a formalism for the specification of the syntax
of programming languages. 
Attribute grammars allow to decorate the nodes of an Abstract Syntax Tree (AST) with attributes. 
The attributes are computed by the attribute grammar rules, i.e., the values are specified
through equations. Reference Attribute Grammars extend Knuth's Attribute Grammars
with reference attributes whose values are references to other AST nodes. 
There are many implementations of Reference Attribute Grammars systems, including
the metacompiler and langauge JastAdd, used to implement large-scale compilers, such as,
ExtendJ and JModelica, and static analysis tools like the one described in this thesis.
Other famous implementations of Reference Attribute Grammars systems are Kiama and Silver, 
the latter being used to implement a C and Java compiler.

In Paper I, we present IntraCFG, a declarative and language-independent framework for 
construcing control-flow graph superimposed on top of the AST. In the same paper, we
present the implementation of a static analysis tool, called IntraJ, that uses IntraCFG
to perform static analysis on Java programs. IntraJ supports two different dataflow analysis:
null-pointer exception analysis and dead assignments detection, that are, a forward and 
backward analysis, respectively. 
In Paper II, we discuss the lack of autmated tools that helps the researcher 
selecting appropriate and relevant corpora for the evaluation of static analysis tools, and more
generally, for the evaluation of any software engineering tool. We present a tool, called
JFeature, that automatically extracts a set of features, divided in different categories,
from Java programs. We used JFeature to extract relevant features introduced in different 
Java versions, and we conducted an empirical study on four well-known Java corpora analysing 
their modernity. We figured out that only one of the four corpora is modern enough to be used
for evaluating Java 8 applications.

 \subsection{Thesis Outline}








